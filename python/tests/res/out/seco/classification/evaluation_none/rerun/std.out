mlrl-testbed mlrl.seco --mode read --log-level debug --base-dir python/tests/res/tmp/rerun --input-dir python/tests/res/tmp --save-evaluation true --print-evaluation true --save-evaluation true
Reading meta-data...
Reading input data from file "python/tests/res/tmp/metadata.yml"...
Successfully read meta-data
Checking for version conflicts...
Experimental results have been created with version "<version>" of the package "mlrl-testbed", version "<version>" is currently used
No version conflicts detected
Reading experimental results of 1 experiment...

Reading experimental results of experiment (1 / 1)...
The command "mlrl-testbed mlrl.seco --log-level debug --base-dir python/tests/res/tmp --data-dir python/tests/res/data --dataset emotions --result-dir results --save-evaluation true --data-split none --print-evaluation true --save-evaluation true" has been used originally for running this experiment
Not using separate training and test sets. The model will be evaluated on the training data...
Reading input data from file "python/tests/res/tmp/results/evaluation_training.csv"...
Evaluation result for training data:

Example-wise F1 (↑)         66.41
Example-wise Jaccard (↑)    57.36
Example-wise Precision (↑)  75.11
Example-wise Recall (↑)     69.59
Hamming Accuracy (↑)        81.51
Hamming Loss (↓)            18.49
Macro F1 (↑)                61.59
Macro Jaccard (↑)           49.23
Macro Precision (↑)         78.61
Macro Recall (↑)            65.96
Micro F1 (↑)                69.84
Micro Jaccard (↑)           53.66
Micro Precision (↑)         70.95
Micro Recall (↑)            68.77
Subset 0/1 Loss (↓)         70.66
Subset Accuracy (↑)         29.34

Writing output data to file "python/tests/res/tmp/rerun/results/evaluation_training.csv"...
