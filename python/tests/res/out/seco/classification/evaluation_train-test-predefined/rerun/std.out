mlrl-testbed mlrl.seco --mode read --log-level debug --base-dir python/tests/res/tmp/rerun --input-dir python/tests/res/tmp --save-evaluation true --print-evaluation true --save-evaluation true
Reading meta-data...
DEBUG: Reading input data from file "python/tests/res/tmp/metadata.yml"...
Successfully read meta-data
DEBUG: Checking for version conflicts...
DEBUG: Experimental results have been created with version "<version>" of the package "mlrl-testbed", version "<version>" is currently used
DEBUG: No version conflicts detected
Reading experimental results of 1 experiment...

Reading experimental results of experiment (1 / 1)...
The command "mlrl-testbed mlrl.seco --log-level debug --base-dir python/tests/res/tmp --data-dir python/tests/res/data --dataset emotions-predefined --result-dir results --save-evaluation true --data-split train-test --print-evaluation true --save-evaluation true" has been used originally for running this experiment
Using separate training and test sets...
DEBUG: Reading input data from file "python/tests/res/tmp/results/evaluation_test.csv"...
Evaluation result for test data:

Example-wise F1 (↑)         52.59
Example-wise Jaccard (↑)    43.14
Example-wise Precision (↑)  64.13
Example-wise Recall (↑)     54.29
Hamming Accuracy (↑)        72.28
Hamming Loss (↓)            27.72
Macro F1 (↑)                49
Macro Jaccard (↑)           34.99
Macro Precision (↑)         67.43
Macro Recall (↑)            51.57
Micro F1 (↑)                55.79
Micro Jaccard (↑)           38.69
Micro Precision (↑)         58.73
Micro Recall (↑)            53.13
Subset 0/1 Loss (↓)         83.17
Subset Accuracy (↑)         16.83

DEBUG: Writing output data to file "python/tests/res/tmp/rerun/results/evaluation_test.csv"...
