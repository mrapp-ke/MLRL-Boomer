mlrl-testbed mlrl.boosting --log-level debug --base-dir python/tests/res/tmp --data-dir python/tests/res/data --dataset emotions --result-dir results --save-evaluation true --marginal-probability-calibration isotonic --print-marginal-probability-calibration-model true --save-marginal-probability-calibration-model true --prediction-type probabilities --probability-predictor output-wise --incremental-evaluation true{step_size=50} --print-evaluation true --save-evaluation true --save-models true --load-models true --model-load-dir models --model-save-dir models
Checking if output files do already exist...
Starting experiment using the classification algorithm "BoomerClassifier"...
Writing output data to file "python/tests/res/tmp/metadata.yml"...
Using separate training and test sets...
Reading input data from file "python/tests/res/data/emotions.arff"...
Parsing meta-data from file "python/tests/res/data/emotions.xml"...
Reading input data from file "models/model.pickle"...
The file "models/model.pickle" does not exist
Fitting model to 397 training examples...
A dense matrix is used to store the feature values of the training examples
A dense matrix is used to store the labels of the training examples
Successfully fit model in <duration>
A dense matrix is used to store the feature values of the query examples
A dense matrix is used to store the predicted probability estimates
Predicting for 196 test examples using a model of size 50...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 50:

Coverage Error (↓)                    3.11
Discounted Cumulative Gain (↑)        1.32
Label Ranking Average Precision (↑)   0.76
Mean Absolute Error (↓)               0.4
Mean Absolute Percentage Error (↓)    1.13208e+15
Mean Squared Error (↓)                0.18
Median Absolute Error (↓)             0.4
NDCG (↑)                             83.92
Ranking Loss (↓)                      0.21

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 100...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 100:

Coverage Error (↓)                    3.01
Discounted Cumulative Gain (↑)        1.32
Label Ranking Average Precision (↑)   0.76
Mean Absolute Error (↓)               0.38
Mean Absolute Percentage Error (↓)    1.05572e+15
Mean Squared Error (↓)                0.17
Median Absolute Error (↓)             0.37
NDCG (↑)                             83.82
Ranking Loss (↓)                      0.2

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 150...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 150:

Coverage Error (↓)                    2.99
Discounted Cumulative Gain (↑)        1.32
Label Ranking Average Precision (↑)   0.76
Mean Absolute Error (↓)               0.37
Mean Absolute Percentage Error (↓)    9.98006e+14
Mean Squared Error (↓)                0.17
Median Absolute Error (↓)             0.35
NDCG (↑)                             83.95
Ranking Loss (↓)                      0.19

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 200...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 200:

Coverage Error (↓)                    2.93
Discounted Cumulative Gain (↑)        1.34
Label Ranking Average Precision (↑)   0.77
Mean Absolute Error (↓)               0.35
Mean Absolute Percentage Error (↓)    9.51229e+14
Mean Squared Error (↓)                0.16
Median Absolute Error (↓)             0.33
NDCG (↑)                             84.73
Ranking Loss (↓)                      0.18

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 250...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 250:

Coverage Error (↓)                    2.9
Discounted Cumulative Gain (↑)        1.34
Label Ranking Average Precision (↑)   0.78
Mean Absolute Error (↓)               0.34
Mean Absolute Percentage Error (↓)    9.09299e+14
Mean Squared Error (↓)                0.16
Median Absolute Error (↓)             0.31
NDCG (↑)                             85.38
Ranking Loss (↓)                      0.18

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 300...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 300:

Coverage Error (↓)                    2.86
Discounted Cumulative Gain (↑)        1.35
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.33
Mean Absolute Percentage Error (↓)    8.79746e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.3
NDCG (↑)                             85.93
Ranking Loss (↓)                      0.17

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 350...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 350:

Coverage Error (↓)                    2.85
Discounted Cumulative Gain (↑)        1.35
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.32
Mean Absolute Percentage Error (↓)    8.53313e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.29
NDCG (↑)                             85.93
Ranking Loss (↓)                      0.17

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 400...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 400:

Coverage Error (↓)                    2.85
Discounted Cumulative Gain (↑)        1.35
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.32
Mean Absolute Percentage Error (↓)    8.32165e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.28
NDCG (↑)                             86.01
Ranking Loss (↓)                      0.17

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 450...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 450:

Coverage Error (↓)                    2.84
Discounted Cumulative Gain (↑)        1.35
Label Ranking Average Precision (↑)   0.8
Mean Absolute Error (↓)               0.31
Mean Absolute Percentage Error (↓)    8.1182e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.27
NDCG (↑)                             86.15
Ranking Loss (↓)                      0.17

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 500...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 500:

Coverage Error (↓)                    2.83
Discounted Cumulative Gain (↑)        1.35
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.31
Mean Absolute Percentage Error (↓)    7.93419e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.27
NDCG (↑)                             85.88
Ranking Loss (↓)                      0.17

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 550...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 550:

Coverage Error (↓)                    2.82
Discounted Cumulative Gain (↑)        1.35
Label Ranking Average Precision (↑)   0.8
Mean Absolute Error (↓)               0.3
Mean Absolute Percentage Error (↓)    7.77731e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.26
NDCG (↑)                             86.16
Ranking Loss (↓)                      0.16

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 600...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 600:

Coverage Error (↓)                    2.83
Discounted Cumulative Gain (↑)        1.35
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.3
Mean Absolute Percentage Error (↓)    7.61673e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.26
NDCG (↑)                             86.09
Ranking Loss (↓)                      0.17

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 650...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 650:

Coverage Error (↓)                    2.82
Discounted Cumulative Gain (↑)        1.36
Label Ranking Average Precision (↑)   0.8
Mean Absolute Error (↓)               0.29
Mean Absolute Percentage Error (↓)    7.52256e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.25
NDCG (↑)                             86.48
Ranking Loss (↓)                      0.16

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 700...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 700:

Coverage Error (↓)                    2.84
Discounted Cumulative Gain (↑)        1.36
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.29
Mean Absolute Percentage Error (↓)    7.40522e+14
Mean Squared Error (↓)                0.15
Median Absolute Error (↓)             0.25
NDCG (↑)                             86
Ranking Loss (↓)                      0.17

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 750...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 750:

Coverage Error (↓)                    2.82
Discounted Cumulative Gain (↑)        1.36
Label Ranking Average Precision (↑)   0.8
Mean Absolute Error (↓)               0.29
Mean Absolute Percentage Error (↓)    7.29848e+14
Mean Squared Error (↓)                0.14
Median Absolute Error (↓)             0.25
NDCG (↑)                             86.54
Ranking Loss (↓)                      0.16

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 800...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 800:

Coverage Error (↓)                    2.84
Discounted Cumulative Gain (↑)        1.36
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.29
Mean Absolute Percentage Error (↓)    7.20322e+14
Mean Squared Error (↓)                0.14
Median Absolute Error (↓)             0.24
NDCG (↑)                             86.1
Ranking Loss (↓)                      0.16

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 850...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 850:

Coverage Error (↓)                    2.84
Discounted Cumulative Gain (↑)        1.36
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.28
Mean Absolute Percentage Error (↓)    7.13406e+14
Mean Squared Error (↓)                0.14
Median Absolute Error (↓)             0.23
NDCG (↑)                             86.11
Ranking Loss (↓)                      0.16

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 900...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 900:

Coverage Error (↓)                    2.83
Discounted Cumulative Gain (↑)        1.36
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.28
Mean Absolute Percentage Error (↓)    7.0715e+14
Mean Squared Error (↓)                0.14
Median Absolute Error (↓)             0.23
NDCG (↑)                             86.24
Ranking Loss (↓)                      0.16

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 950...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 950:

Coverage Error (↓)                    2.84
Discounted Cumulative Gain (↑)        1.36
Label Ranking Average Precision (↑)   0.79
Mean Absolute Error (↓)               0.28
Mean Absolute Percentage Error (↓)    7.0078e+14
Mean Squared Error (↓)                0.14
Median Absolute Error (↓)             0.22
NDCG (↑)                             86.24
Ranking Loss (↓)                      0.16

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Predicting for 196 test examples using a model of size 1000...
Successfully predicted in <duration>
Evaluation result for test data using a model of size 1000:

Coverage Error (↓)                    2.83
Discounted Cumulative Gain (↑)        1.36
Label Ranking Average Precision (↑)   0.8
Mean Absolute Error (↓)               0.28
Mean Absolute Percentage Error (↓)    6.96985e+14
Mean Squared Error (↓)                0.14
Median Absolute Error (↓)             0.22
NDCG (↑)                             86.27
Ranking Loss (↓)                      0.16

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Marginal probability calibration model:

┌──────────────────────┬─────────────────────────┐
│   Label 1 thresholds │   Label 1 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0006 │                  0.0769 │
│               0.0023 │                  0.0952 │
│               0.0212 │                  0.3042 │
│               0.1312 │                  0.4    │
│               0.2105 │                  0.4417 │
│               0.5997 │                  0.5    │
│               0.6951 │                  0.6333 │
│               0.8326 │                  0.6667 │
│               0.8922 │                  0.875  │
│               0.9603 │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 2 thresholds │   Label 2 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0022 │                  0.2525 │
│               0.0162 │                  0.2694 │
│               0.086  │                  0.3333 │
│               0.1316 │                  0.3479 │
│               0.2913 │                  0.5    │
│               0.3338 │                  0.5238 │
│               0.6828 │                  0.8036 │
│               0.9646 │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 3 thresholds │   Label 3 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0408 │                  0.3333 │
│               0.0923 │                  0.5    │
│               0.5461 │                  0.5042 │
│               0.9957 │                  0.6667 │
│               0.997  │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 4 thresholds │   Label 4 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0003 │                  0.0159 │
│               0.0483 │                  0.2    │
│               0.3111 │                  0.3333 │
│               0.518  │                  0.4167 │
│               0.9344 │                  0.7333 │
│               0.9868 │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 5 thresholds │   Label 5 probabilities │
├──────────────────────┼─────────────────────────┤
│               0.0001 │                  0      │
│               0.0031 │                  0.1849 │
│               0.0202 │                  0.2    │
│               0.0267 │                  0.2306 │
│               0.0679 │                  0.25   │
│               0.1198 │                  0.2946 │
│               0.7024 │                  0.5    │
│               0.953  │                  0.8    │
│               0.9754 │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 6 thresholds │   Label 6 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0011 │                  0.0286 │
│               0.0138 │                  0.1429 │
│               0.0186 │                  0.25   │
│               0.1785 │                  0.5    │
│               0.2218 │                  0.5208 │
│               0.665  │                  0.5312 │
│               0.9485 │                  0.6786 │
│               0.9875 │                  1      │
└──────────────────────┴─────────────────────────┘

Writing output data to file "python/tests/res/tmp/results/marginal_probability_calibration_model.csv"...
Writing output data to file "python/tests/res/tmp/models/model.pickle"...
Successfully finished experiment after <duration>
