mlrl-testbed mlrl.boosting --mode batch --log-level debug --base-dir python/tests/res/tmp --config python/tests/res/config/boosting/classification/batch_config.yml --runner slurm --slurm-config python/tests/res/config/slurm_config.yml --print-slurm-scripts true --save-slurm-scripts true --slurm-save-dir python/tests/res/tmp --save-evaluation true --data-split cross-validation{num_folds=2}
Writing output data to file "python/tests/res/tmp/metadata.yml"...
Submitting 16 experiments to Slurm...

Submitting Slurm job (1 / 8): "mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2} --dataset emotions --instance-sampling none --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_none/loss_logistic-decomposable/dataset_emotions/models --parameter-save-dir instance-sampling_none/loss_logistic-decomposable/dataset_emotions/parameters --result-dir instance-sampling_none/loss_logistic-decomposable/dataset_emotions/results --save-evaluation true --save-meta-data false --wipe-result-dir false"
Slurm script saved to file "python/tests/res/tmp/sbatch_1.sh"
Content of Slurm script is:

#!/bin/sh

#SBATCH --output=python/tests/res/tmp/instance-sampling_none/loss_logistic-decomposable/dataset_emotions/results/std_fold-%a.out
#SBATCH --error=python/tests/res/tmp/instance-sampling_none/loss_logistic-decomposable/dataset_emotions/results/std_fold-%a.err
#SBATCH --array=1-2
#SBATCH --time=2:30:00
#SBATCH --cpus-per-tasks=1

python -m venv .venv
. .venv/bin/activate
python -m pip install mlrl-testbed-sklearn
mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation'{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2}' --dataset emotions --instance-sampling none --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_none/loss_logistic-decomposable/dataset_emotions/models --parameter-save-dir instance-sampling_none/loss_logistic-decomposable/dataset_emotions/parameters --result-dir instance-sampling_none/loss_logistic-decomposable/dataset_emotions/results --save-evaluation true --save-meta-data false --wipe-result-dir false
deactivate


Submitting Slurm job (2 / 8): "mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2} --dataset emotions --feature-format sparse --instance-sampling none --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/models --parameter-save-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/parameters --result-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/results --save-evaluation true --save-meta-data false --wipe-result-dir false"
Slurm script saved to file "python/tests/res/tmp/sbatch_2.sh"
Content of Slurm script is:

#!/bin/sh

#SBATCH --output=python/tests/res/tmp/instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/results/std_fold-%a.out
#SBATCH --error=python/tests/res/tmp/instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/results/std_fold-%a.err
#SBATCH --array=1-2
#SBATCH --time=2:30:00
#SBATCH --cpus-per-tasks=1

python -m venv .venv
. .venv/bin/activate
python -m pip install mlrl-testbed-sklearn
mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation'{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2}' --dataset emotions --feature-format sparse --instance-sampling none --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/models --parameter-save-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/parameters --result-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/results --save-evaluation true --save-meta-data false --wipe-result-dir false
deactivate


Submitting Slurm job (3 / 8): "mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2} --dataset emotions --instance-sampling with-replacement{sample_size=0.5} --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_emotions/models --parameter-save-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_emotions/parameters --result-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_emotions/results --save-evaluation true --save-meta-data false --wipe-result-dir false"
Slurm script saved to file "python/tests/res/tmp/sbatch_3.sh"
Content of Slurm script is:

#!/bin/sh

#SBATCH --output=python/tests/res/tmp/instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_emotions/results/std_fold-%a.out
#SBATCH --error=python/tests/res/tmp/instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_emotions/results/std_fold-%a.err
#SBATCH --array=1-2
#SBATCH --time=2:30:00
#SBATCH --cpus-per-tasks=1

python -m venv .venv
. .venv/bin/activate
python -m pip install mlrl-testbed-sklearn
mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation'{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2}' --dataset emotions --instance-sampling with-replacement'{sample_size=0.5}' --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/dataset_emotions/models --parameter-save-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/dataset_emotions/parameters --result-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/dataset_emotions/results --save-evaluation true --save-meta-data false --wipe-result-dir false
deactivate


Submitting Slurm job (4 / 8): "mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2} --dataset emotions --feature-format sparse --instance-sampling with-replacement{sample_size=0.5} --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/models --parameter-save-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/parameters --result-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/results --save-evaluation true --save-meta-data false --wipe-result-dir false"
Slurm script saved to file "python/tests/res/tmp/sbatch_4.sh"
Content of Slurm script is:

#!/bin/sh

#SBATCH --output=python/tests/res/tmp/instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/results/std_fold-%a.out
#SBATCH --error=python/tests/res/tmp/instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/results/std_fold-%a.err
#SBATCH --array=1-2
#SBATCH --time=2:30:00
#SBATCH --cpus-per-tasks=1

python -m venv .venv
. .venv/bin/activate
python -m pip install mlrl-testbed-sklearn
mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation'{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2}' --dataset emotions --feature-format sparse --instance-sampling with-replacement'{sample_size=0.5}' --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/models --parameter-save-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/parameters --result-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/feature-format_sparse/dataset_emotions/results --save-evaluation true --save-meta-data false --wipe-result-dir false
deactivate


Submitting Slurm job (5 / 8): "mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2} --dataset enron --instance-sampling none --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_none/loss_logistic-decomposable/dataset_enron/models --parameter-save-dir instance-sampling_none/loss_logistic-decomposable/dataset_enron/parameters --result-dir instance-sampling_none/loss_logistic-decomposable/dataset_enron/results --save-evaluation true --save-meta-data false --wipe-result-dir false"
Slurm script saved to file "python/tests/res/tmp/sbatch_5.sh"
Content of Slurm script is:

#!/bin/sh

#SBATCH --output=python/tests/res/tmp/instance-sampling_none/loss_logistic-decomposable/dataset_enron/results/std_fold-%a.out
#SBATCH --error=python/tests/res/tmp/instance-sampling_none/loss_logistic-decomposable/dataset_enron/results/std_fold-%a.err
#SBATCH --array=1-2
#SBATCH --time=2:30:00
#SBATCH --cpus-per-tasks=1

python -m venv .venv
. .venv/bin/activate
python -m pip install mlrl-testbed-sklearn
mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation'{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2}' --dataset enron --instance-sampling none --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_none/loss_logistic-decomposable/dataset_enron/models --parameter-save-dir instance-sampling_none/loss_logistic-decomposable/dataset_enron/parameters --result-dir instance-sampling_none/loss_logistic-decomposable/dataset_enron/results --save-evaluation true --save-meta-data false --wipe-result-dir false
deactivate


Submitting Slurm job (6 / 8): "mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2} --dataset enron --feature-format sparse --instance-sampling none --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_enron/models --parameter-save-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_enron/parameters --result-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_enron/results --save-evaluation true --save-meta-data false --wipe-result-dir false"
Slurm script saved to file "python/tests/res/tmp/sbatch_6.sh"
Content of Slurm script is:

#!/bin/sh

#SBATCH --output=python/tests/res/tmp/instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_enron/results/std_fold-%a.out
#SBATCH --error=python/tests/res/tmp/instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_enron/results/std_fold-%a.err
#SBATCH --array=1-2
#SBATCH --time=2:30:00
#SBATCH --cpus-per-tasks=1

python -m venv .venv
. .venv/bin/activate
python -m pip install mlrl-testbed-sklearn
mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation'{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2}' --dataset enron --feature-format sparse --instance-sampling none --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_enron/models --parameter-save-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_enron/parameters --result-dir instance-sampling_none/loss_logistic-decomposable/feature-format_sparse/dataset_enron/results --save-evaluation true --save-meta-data false --wipe-result-dir false
deactivate


Submitting Slurm job (7 / 8): "mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2} --dataset enron --instance-sampling with-replacement{sample_size=0.5} --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_enron/models --parameter-save-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_enron/parameters --result-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_enron/results --save-evaluation true --save-meta-data false --wipe-result-dir false"
Slurm script saved to file "python/tests/res/tmp/sbatch_7.sh"
Content of Slurm script is:

#!/bin/sh

#SBATCH --output=python/tests/res/tmp/instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_enron/results/std_fold-%a.out
#SBATCH --error=python/tests/res/tmp/instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/dataset_enron/results/std_fold-%a.err
#SBATCH --array=1-2
#SBATCH --time=2:30:00
#SBATCH --cpus-per-tasks=1

python -m venv .venv
. .venv/bin/activate
python -m pip install mlrl-testbed-sklearn
mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation'{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2}' --dataset enron --instance-sampling with-replacement'{sample_size=0.5}' --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/dataset_enron/models --parameter-save-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/dataset_enron/parameters --result-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/dataset_enron/results --save-evaluation true --save-meta-data false --wipe-result-dir false
deactivate


Submitting Slurm job (8 / 8): "mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2} --dataset enron --feature-format sparse --instance-sampling with-replacement{sample_size=0.5} --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_enron/models --parameter-save-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_enron/parameters --result-dir instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_enron/results --save-evaluation true --save-meta-data false --wipe-result-dir false"
Slurm script saved to file "python/tests/res/tmp/sbatch_8.sh"
Content of Slurm script is:

#!/bin/sh

#SBATCH --output=python/tests/res/tmp/instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_enron/results/std_fold-%a.out
#SBATCH --error=python/tests/res/tmp/instance-sampling_with-replacement{sample_size=0.5}/loss_logistic-decomposable/feature-format_sparse/dataset_enron/results/std_fold-%a.err
#SBATCH --array=1-2
#SBATCH --time=2:30:00
#SBATCH --cpus-per-tasks=1

python -m venv .venv
. .venv/bin/activate
python -m pip install mlrl-testbed-sklearn
mlrl-testbed mlrl.boosting --base-dir python/tests/res/tmp --data-dir python/tests/res/data --data-split cross-validation'{first_fold=$SLURM_ARRAY_TASK_ID,last_fold=$SLURM_ARRAY_TASK_ID,num_folds=2}' --dataset enron --feature-format sparse --instance-sampling with-replacement'{sample_size=0.5}' --log-level debug --loss logistic-decomposable --model-save-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/feature-format_sparse/dataset_enron/models --parameter-save-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/feature-format_sparse/dataset_enron/parameters --result-dir instance-sampling_with-replacement'{sample_size=0.5}'/loss_logistic-decomposable/feature-format_sparse/dataset_enron/results --save-evaluation true --save-meta-data false --wipe-result-dir false
deactivate

