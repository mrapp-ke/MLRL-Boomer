mlrl-testbed mlrl.boosting --log-level debug --base-dir python/tests/res/tmp --data-dir python/tests/res/data --dataset emotions --result-dir results --save-evaluation true --predict-for-training-data true --print-evaluation true --save-evaluation true
Starting experiment using the classification algorithm "BoomerClassifier"...
Writing output data to file "python/tests/res/tmp/metadata.yml"...
Using separate training and test sets...
Reading input data from file "python/tests/res/data/emotions.arff"...
Parsing meta-data from file "python/tests/res/data/emotions.xml"...
Fitting model to 397 training examples...
A dense matrix is used to store the feature values of the training examples
A dense matrix is used to store the labels of the training examples
Successfully fit model in <duration>
Predicting for 397 training examples...
A dense matrix is used to store the feature values of the query examples
A dense matrix is used to store the predicted labels
Successfully predicted in <duration>
Evaluation result for training data:

Example-wise F1 (↑)         100
Example-wise Jaccard (↑)    100
Example-wise Precision (↑)  100
Example-wise Recall (↑)     100
Hamming Accuracy (↑)        100
Hamming Loss (↓)              0
Macro F1 (↑)                100
Macro Jaccard (↑)           100
Macro Precision (↑)         100
Macro Recall (↑)            100
Micro F1 (↑)                100
Micro Jaccard (↑)           100
Micro Precision (↑)         100
Micro Recall (↑)            100
Subset 0/1 Loss (↓)           0
Subset Accuracy (↑)         100

Writing output data to file "python/tests/res/tmp/results/evaluation_training.csv"...
Predicting for 196 test examples...
A dense matrix is used to store the feature values of the query examples
A dense matrix is used to store the predicted labels
Successfully predicted in <duration>
Evaluation result for test data:

Example-wise F1 (↑)         63.11
Example-wise Jaccard (↑)    55.31
Example-wise Precision (↑)  78.06
Example-wise Recall (↑)     62.67
Hamming Accuracy (↑)        81.29
Hamming Loss (↓)            18.71
Macro F1 (↑)                65.05
Macro Jaccard (↑)           49.47
Macro Precision (↑)         75.75
Macro Recall (↑)            60.43
Micro F1 (↑)                67.84
Micro Jaccard (↑)           51.33
Micro Precision (↑)         75.08
Micro Recall (↑)            61.87
Subset 0/1 Loss (↓)         69.39
Subset Accuracy (↑)         30.61

Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
Successfully finished experiment after <duration>
