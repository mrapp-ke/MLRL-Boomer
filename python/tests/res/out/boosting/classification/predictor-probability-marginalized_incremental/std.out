mlrl-testbed mlrl.boosting --log-level debug --data-dir python/tests/res/data --dataset emotions --output-dir python/tests/res/tmp/results --marginal-probability-calibration isotonic --print-marginal-probability-calibration-model true --store-marginal-probability-calibration-model true --joint-probability-calibration isotonic --print-joint-probability-calibration-model true --store-joint-probability-calibration-model true --prediction-type probabilities --probability-predictor marginalized --incremental-evaluation true{step_size=50} --print-evaluation true --store-evaluation true --model-load-dir python/tests/res/tmp/models --model-save-dir python/tests/res/tmp/models
INFO Starting experiment using the classification algorithm "BoomerClassifier"...
INFO Using separate training and test sets...
DEBUG Reading input data from file "python/tests/res/data/emotions.arff"...
DEBUG Parsing meta-data from file "python/tests/res/data/emotions.xml"...
DEBUG Reading input data from file "python/tests/res/tmp/models/model.pickle"...
INFO Successfully loaded model
DEBUG A dense matrix is used to store the feature values of the query examples
DEBUG A dense matrix is used to store the predicted probability estimates
INFO Predicting for 196 test examples using a model of size 50...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 50:

Coverage Error                    3.06
Discounted Cumulative Gain        1.33
Label Ranking Average Precision   0.77
Mean Absolute Error               0.36
Mean Absolute Percentage Error    7.62326e+14
Mean Squared Error                0.17
Median Absolute Error             0.3
NDCG                             84.43
Ranking Loss                      0.2

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 100...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 100:

Coverage Error                    2.97
Discounted Cumulative Gain        1.35
Label Ranking Average Precision   0.78
Mean Absolute Error               0.35
Mean Absolute Percentage Error    7.29705e+14
Mean Squared Error                0.17
Median Absolute Error             0.3
NDCG                             85.31
Ranking Loss                      0.19

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 150...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 150:

Coverage Error                    2.94
Discounted Cumulative Gain        1.36
Label Ranking Average Precision   0.79
Mean Absolute Error               0.34
Mean Absolute Percentage Error    7.04351e+14
Mean Squared Error                0.16
Median Absolute Error             0.29
NDCG                             86.15
Ranking Loss                      0.18

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 200...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 200:

Coverage Error                    2.99
Discounted Cumulative Gain        1.35
Label Ranking Average Precision   0.78
Mean Absolute Error               0.34
Mean Absolute Percentage Error    6.73697e+14
Mean Squared Error                0.17
Median Absolute Error             0.28
NDCG                             85.77
Ranking Loss                      0.2

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 250...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 250:

Coverage Error                    3.04
Discounted Cumulative Gain        1.35
Label Ranking Average Precision   0.78
Mean Absolute Error               0.34
Mean Absolute Percentage Error    6.38364e+14
Mean Squared Error                0.17
Median Absolute Error             0.27
NDCG                             85.89
Ranking Loss                      0.22

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 300...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 300:

Coverage Error                    3.18
Discounted Cumulative Gain        1.33
Label Ranking Average Precision   0.75
Mean Absolute Error               0.34
Mean Absolute Percentage Error    6.07306e+14
Mean Squared Error                0.18
Median Absolute Error             0.27
NDCG                             84.65
Ranking Loss                      0.26

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 350...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 350:

Coverage Error                    3.2
Discounted Cumulative Gain        1.33
Label Ranking Average Precision   0.75
Mean Absolute Error               0.33
Mean Absolute Percentage Error    5.94564e+14
Mean Squared Error                0.18
Median Absolute Error             0.26
NDCG                             84.79
Ranking Loss                      0.26

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 400...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 400:

Coverage Error                    3.18
Discounted Cumulative Gain        1.32
Label Ranking Average Precision   0.75
Mean Absolute Error               0.33
Mean Absolute Percentage Error    5.84283e+14
Mean Squared Error                0.18
Median Absolute Error             0.26
NDCG                             84.72
Ranking Loss                      0.26

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 450...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 450:

Coverage Error                    3.31
Discounted Cumulative Gain        1.31
Label Ranking Average Precision   0.73
Mean Absolute Error               0.33
Mean Absolute Percentage Error    5.6839e+14
Mean Squared Error                0.19
Median Absolute Error             0.26
NDCG                             83.45
Ranking Loss                      0.29

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 500...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 500:

Coverage Error                    3.37
Discounted Cumulative Gain        1.3
Label Ranking Average Precision   0.72
Mean Absolute Error               0.33
Mean Absolute Percentage Error    5.53997e+14
Mean Squared Error                0.19
Median Absolute Error             0.26
NDCG                             82.87
Ranking Loss                      0.31

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 550...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 550:

Coverage Error                    3.4
Discounted Cumulative Gain        1.29
Label Ranking Average Precision   0.71
Mean Absolute Error               0.33
Mean Absolute Percentage Error    5.43335e+14
Mean Squared Error                0.2
Median Absolute Error             0.26
NDCG                             82.64
Ranking Loss                      0.32

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 600...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 600:

Coverage Error                    3.43
Discounted Cumulative Gain        1.29
Label Ranking Average Precision   0.71
Mean Absolute Error               0.33
Mean Absolute Percentage Error    5.34197e+14
Mean Squared Error                0.2
Median Absolute Error             0.25
NDCG                             82.72
Ranking Loss                      0.33

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 650...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 650:

Coverage Error                    3.52
Discounted Cumulative Gain        1.28
Label Ranking Average Precision   0.7
Mean Absolute Error               0.33
Mean Absolute Percentage Error    5.16585e+14
Mean Squared Error                0.2
Median Absolute Error             0.24
NDCG                             82.01
Ranking Loss                      0.35

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 700...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 700:

Coverage Error                    3.61
Discounted Cumulative Gain        1.27
Label Ranking Average Precision   0.68
Mean Absolute Error               0.33
Mean Absolute Percentage Error    5.01137e+14
Mean Squared Error                0.21
Median Absolute Error             0.24
NDCG                             81.31
Ranking Loss                      0.38

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 750...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 750:

Coverage Error                    3.66
Discounted Cumulative Gain        1.27
Label Ranking Average Precision   0.67
Mean Absolute Error               0.33
Mean Absolute Percentage Error    4.92842e+14
Mean Squared Error                0.21
Median Absolute Error             0.24
NDCG                             80.72
Ranking Loss                      0.39

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 800...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 800:

Coverage Error                    3.68
Discounted Cumulative Gain        1.26
Label Ranking Average Precision   0.67
Mean Absolute Error               0.33
Mean Absolute Percentage Error    4.85737e+14
Mean Squared Error                0.21
Median Absolute Error             0.24
NDCG                             80.65
Ranking Loss                      0.4

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 850...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 850:

Coverage Error                    3.77
Discounted Cumulative Gain        1.26
Label Ranking Average Precision   0.66
Mean Absolute Error               0.33
Mean Absolute Percentage Error    4.75656e+14
Mean Squared Error                0.22
Median Absolute Error             0.23
NDCG                             80.19
Ranking Loss                      0.42

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 900...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 900:

Coverage Error                    3.88
Discounted Cumulative Gain        1.25
Label Ranking Average Precision   0.64
Mean Absolute Error               0.33
Mean Absolute Percentage Error    4.53008e+14
Mean Squared Error                0.22
Median Absolute Error             0.23
NDCG                             79.37
Ranking Loss                      0.44

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 950...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 950:

Coverage Error                    3.9
Discounted Cumulative Gain        1.25
Label Ranking Average Precision   0.64
Mean Absolute Error               0.33
Mean Absolute Percentage Error    4.4578e+14
Mean Squared Error                0.22
Median Absolute Error             0.23
NDCG                             79.56
Ranking Loss                      0.44

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
INFO Predicting for 196 test examples using a model of size 1000...
INFO Successfully predicted in <duration>
INFO Evaluation result for test data using a model of size 1000:

Coverage Error                    3.94
Discounted Cumulative Gain        1.25
Label Ranking Average Precision   0.63
Mean Absolute Error               0.33
Mean Absolute Percentage Error    4.40522e+14
Mean Squared Error                0.22
Median Absolute Error             0.22
NDCG                             79.2
Ranking Loss                      0.45

DEBUG Writing output data to file "python/tests/res/tmp/results/evaluation_test.csv"...
DEBUG Writing output data to file "python/tests/res/tmp/models/model.pickle"...
INFO Marginal probability calibration model:

┌──────────────────────┬─────────────────────────┐
│   Label 1 thresholds │   Label 1 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0012 │                  0.2441 │
│               0.026  │                  0.3312 │
│               0.2426 │                  0.4333 │
│               0.404  │                  0.5    │
│               0.7181 │                  0.6094 │
│               0.9848 │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 2 thresholds │   Label 2 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0015 │                  0.2    │
│               0.0019 │                  0.2097 │
│               0.0446 │                  0.2381 │
│               0.0657 │                  0.375  │
│               0.1106 │                  0.3823 │
│               0.3771 │                  0.5    │
│               0.7206 │                  0.7045 │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 3 thresholds │   Label 3 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0285 │                  0.2    │
│               0.0973 │                  0.5226 │
│               0.9807 │                  0.5655 │
│               0.9983 │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 4 thresholds │   Label 4 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0003 │                  0.0156 │
│               0.0657 │                  0.2667 │
│               0.3108 │                  0.3625 │
│               0.9412 │                  0.7083 │
│               0.9847 │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 5 thresholds │   Label 5 probabilities │
├──────────────────────┼─────────────────────────┤
│               0.0001 │                  0      │
│               0.0018 │                  0.1429 │
│               0.0034 │                  0.2162 │
│               0.044  │                  0.25   │
│               0.0566 │                  0.294  │
│               0.5818 │                  0.3125 │
│               0.9535 │                  1      │
└──────────────────────┴─────────────────────────┘
┌──────────────────────┬─────────────────────────┐
│   Label 6 thresholds │   Label 6 probabilities │
├──────────────────────┼─────────────────────────┤
│               0      │                  0      │
│               0.0008 │                  0.025  │
│               0.0129 │                  0.1667 │
│               0.0244 │                  0.25   │
│               0.2021 │                  0.4664 │
│               0.9622 │                  0.875  │
│               0.9862 │                  1      │
└──────────────────────┴─────────────────────────┘

DEBUG Writing output data to file "python/tests/res/tmp/results/marginal_probability_calibration_model.csv"...
INFO Successfully finished after <duration>
