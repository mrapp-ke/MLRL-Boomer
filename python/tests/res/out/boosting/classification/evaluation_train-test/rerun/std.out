mlrl-testbed mlrl.boosting --mode read --log-level debug --base-dir python/tests/res/tmp/rerun --input-dir python/tests/res/tmp --save-evaluation true --print-evaluation true --save-evaluation true
Reading meta-data...
Reading input data from file "python/tests/res/tmp/metadata.yml"...
Successfully read meta-data
Checking for version conflicts...
Experimental results have been created with version "<version>" of the package "mlrl-testbed", version "<version>" is currently used
No version conflicts detected
Reading experimental results of 1 experiment...

Reading experimental results of experiment (1 / 1)...
The command "mlrl-testbed mlrl.boosting --log-level debug --base-dir python/tests/res/tmp --data-dir python/tests/res/data --dataset emotions --result-dir results --save-evaluation true --data-split train-test --print-evaluation true --save-evaluation true" has been used originally for running this experiment
Using separate training and test sets...
Reading input data from file "python/tests/res/tmp/results/evaluation_test.csv"...
Evaluation result for test data:

Example-wise F1         63.11
Example-wise Jaccard    55.31
Example-wise Precision  78.06
Example-wise Recall     62.67
Hamming Accuracy        81.29
Hamming Loss            18.71
Macro F1                65.05
Macro Jaccard           49.47
Macro Precision         75.75
Macro Recall            60.43
Micro F1                67.84
Micro Jaccard           51.33
Micro Precision         75.08
Micro Recall            61.87
Subset 0/1 Loss         69.39
Subset Accuracy         30.61

Writing output data to file "python/tests/res/tmp/rerun/results/evaluation_test.csv"...
